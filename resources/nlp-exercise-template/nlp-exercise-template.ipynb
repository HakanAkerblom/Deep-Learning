{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c19d029-25f5-4d8e-9514-bf3ad9d30026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hakan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hakan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hakan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mc:\\Users\\hakan\\.virtualenvs\\Deep-learning-lab-O8p5G3Ji\\lib\\site-packages\\tensorflow\\__init__.py:54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\hakan\\.virtualenvs\\Deep-learning-lab-O8p5G3Ji\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hakan\\.virtualenvs\\Deep-learning-lab-O8p5G3Ji\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\hakan\\.virtualenvs\\Deep-learning-lab-O8p5G3Ji\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible \u001b[38;5;66;03m# line: 65\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hakan\\.virtualenvs\\Deep-learning-lab-O8p5G3Ji\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\hakan\\.virtualenvs\\Deep-learning-lab-O8p5G3Ji\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\__init__.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\hakan\\.virtualenvs\\Deep-learning-lab-O8p5G3Ji\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2401a-3b28-4a42-ad99-4623e795c5b3",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5766eb4-ca56-44be-9c47-23a102b3d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('imdb_train_data_small.csv')\n",
    "test_df = pd.read_csv('imdb_test_data_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6342a5a9-2277-422f-96af-351bad324de7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a8840-336c-4b42-b6b8-2c4cc53f229f",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Create your own tokenization algorithm. Remember to handle upper/lower case, comma, punctioation and so on.\n",
    "Each word should hava an integer connected to it. Word as key and integer as value in a dict is one way to do it.\n",
    "\n",
    "Tensorflow have tokenization models, but try to bild it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2cc1d-9051-4db9-8fc5-a939df97a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset):\n",
    "    token_map = {}\n",
    "    reverse_token_map = {}\n",
    "    # Your code\n",
    "\n",
    "    token_map['<UNK>'] = 0\n",
    "    return token_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2566c0-015a-4cdf-b88d-b41376df220d",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff2b32-2e36-4848-8be9-c7e880d3ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd737e-3810-4866-8054-5db39ecddace",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32681497-e3e4-44f1-a335-6fce79310f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text)\n",
    "    # Your code\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae56ad-156e-4a0d-be28-d17914bcc475",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77df94d-37e1-4615-a8cf-051296763a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ac992-e0c9-4e47-836a-c7a07ee22aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Horses'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('Horses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82697754-3845-4cbd-8f21-9b8624612c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    # Your code\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0f1fe-e10c-4bc1-8533-b22f7b194f3b",
   "metadata": {},
   "source": [
    "# Word embedding and sentiment analysis model\n",
    "We want to create a model that can say if a movie review is bad or good.\n",
    "\n",
    "- Preprocess the text\n",
    "- Convert text to seqiuence of integers\n",
    "- Create architecture that includes embeddings\n",
    "- Build and train your models\n",
    "- Evaluate preformance\n",
    "\n",
    "Building models from scratch is not something you usually do, but those who would like to dig deeper into the math behind Simple RNN, LSTM and GRU can do it by creating the cells from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67861b1-61e9-4b7f-a525-f27c11093338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(embedded_text):\n",
    "    # All sentences should be of the same lenght, but if a sentence is shorter than the longest, pad it.\n",
    "    return padded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3425e-214e-4f0d-89fa-dd61faacbda3",
   "metadata": {},
   "source": [
    "## RNN with tensorflow modules\n",
    "[Simple RNN cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)\n",
    "\n",
    "[Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac733a4c-b314-4b2e-ae0d-295e3e2671d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01152d72-7062-4d00-b3f4-7d1ae3b30f3e",
   "metadata": {},
   "source": [
    "## RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7e11b-131c-49b7-9a3c-0fcd363a632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.Wxh =\n",
    "        self.Whh =\n",
    "        self.bh =\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        h_next = \n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177958f-cbe0-466c-b39e-78a87ce4f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model Class\n",
    "class MyRNNModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, sequence_length=100):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.rnn_cell = RNNCell(embedding_dim, hidden_dim)\n",
    "        self.Why = \n",
    "        self.by = \n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = \n",
    "        h = \n",
    "\n",
    "        # Process the input sequence\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.rnn_cell(x_t, h)\n",
    "\n",
    "        y = \n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433047d-199f-4aab-858c-f4e25bba4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ececbef-f1f6-4d36-97ee-42ce80b3ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11f54b-f41a-4f5a-b1f4-5c5390d9bb6a",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "[LSTM Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4902ef9-520c-4975-9aff-f89b623104e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198898ec-771e-4b89-b6dc-12944f0c6d19",
   "metadata": {},
   "source": [
    "## LSTM from scrtch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a92a1-e930-41c5-8625-4b6c32adf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Cell Class\n",
    "class LSTMCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Gates: input, forget, cell, output\n",
    "        self.Wi =\n",
    "        self.Wf =\n",
    "        self.Wc =\n",
    "        self.Wo =\n",
    "        self.bi =\n",
    "        self.bf =\n",
    "        self.bc =\n",
    "        self.bo =\n",
    "\n",
    "    def __call__(self, x, h, c):\n",
    "        combined = tf.concat([x, h], 1)\n",
    "\n",
    "        i = \n",
    "        f = \n",
    "        o = \n",
    "        c_ = \n",
    "\n",
    "        c_new = \n",
    "        h_new =\n",
    "\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f3545-5a8f-45d7-813e-075ae61dd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Class\n",
    "class MyLSTMModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.lstm_cell = LSTMCell(embedding_dim, hidden_dim)\n",
    "        self.Why =\n",
    "        self.by =\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x =\n",
    "        h =\n",
    "        c =\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h, c = self.lstm_cell(x_t, h, c)\n",
    "\n",
    "        y =\n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d248f5-0b90-42f7-99ce-351167d00b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    clipped_gradients = [tf.clip_by_norm(g, clip_norm) for g in gradients]\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d6d6e-48d0-4b40-b402-f81b13b62c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3810e-0ae8-4058-b749-799b254f3e81",
   "metadata": {},
   "source": [
    "## GRU\n",
    "[GRU Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da681b6-0ae3-4459-87f4-c8184009ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a06ce2-4f0e-4fb8-b42a-d307982da693",
   "metadata": {},
   "source": [
    "## GRU from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae6a39-8185-4260-9d6b-e1bb45d2b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Cell Class\n",
    "class GRUCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Update gate parameters\n",
    "        self.Wz =\n",
    "        self.bz =\n",
    "\n",
    "        # Reset gate parameters\n",
    "        self.Wr =\n",
    "        self.br =\n",
    "\n",
    "        # Candidate hidden state parameters\n",
    "        self.Wh =\n",
    "        self.bh =\n",
    "        \n",
    "    def __call__(self, x, h):\n",
    "        combined = tf.concat([x, h], 1)\n",
    "\n",
    "        # Update gate\n",
    "        z =\n",
    "\n",
    "        # Reset gate\n",
    "        r =\n",
    "\n",
    "        # Candidate hidden state\n",
    "        combined_reset =\n",
    "        h_candidate =\n",
    "\n",
    "        # New hidden state\n",
    "        h_new =\n",
    "\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6bb3-7353-4aa4-ab88-bf008934661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model Class\n",
    "class MyGRUModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.gru_cell =\n",
    "        self.Why =\n",
    "        self.by =\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x =\n",
    "        h =\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.gru_cell(x_t, h)\n",
    "\n",
    "        y =\n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653c0e5-4d7c-443a-9bce-566c92fcac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3eae4-cda1-4574-bc05-43869b94fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Learning-Zv8C6_EQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
